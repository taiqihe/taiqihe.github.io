---
---


@article{https://doi.org/10.1111/psyp.13976,
author = {He, Taiqi and Boudewyn, Megan A. and Kiat, John E. and Sagae, Kenji and Luck, Steven J.},
title = {Neural correlates of word representation vectors in natural language processing models: Evidence from representational similarity analysis of event-related brain potentials},
journal = {Psychophysiology},
volume = {59},
number = {3},
pages = {e13976},
keywords = {deep neural networks, ERPs, machine learning, methods, representational similarity analysis, RSA, word embeddings},
doi = {https://doi.org/10.1111/psyp.13976},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/psyp.13976},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/psyp.13976},
abstract = {Abstract Natural language processing models based on machine learning (ML-NLP models) have been developed to solve practical problems, such as interpreting an Internet search query. These models are not intended to reflect human language comprehension mechanisms, and the word representations used by ML-NLP models and human brains might therefore be quite different. However, because ML-NLP models are trained with the same kinds of inputs that humans must process, and they must solve many of the same computational problems as the human brain, ML-NLP models and human brains may end up with similar word representations. To distinguish between these hypotheses, we used representational similarity analysis to compare the representational geometry of word representations in two ML-NLP models with the representational geometry of the human brain, as indexed with event-related potentials (ERPs). Participants listened to stories while the electroencephalogram was recorded. We extracted averaged ERPs for each of the 100 words that occurred most frequently in the stories, and we calculated the similarity of the neural response for each pair of words. We compared this 100 × 100 similarity matrix to the 100 × 100 similarity matrix for the word pairs according to two ML-NLP models. We found significant representational similarity between the neural data and each ML-NLP model, beginning within 250 ms of word onset. These results indicate that ML-NLP systems that are designed to solve practical technology problems have a representational geometry that is correlated with that of the human brain, presumably because both are influenced by the structural properties and statistics of language.},
year = {2022},
selected = true,
abbr = {Psychophysiology},
}


@inproceedings{yu-etal-2021-language,
    title = "Language Embeddings for Typology and Cross-lingual Transfer Learning",
    author = "Yu, Dian  and
      He, Taiqi  and
      Sagae, Kenji",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.560",
    doi = "10.18653/v1/2021.acl-long.560",
    pages = "7210--7225",
    abstract = "Cross-lingual language tasks typically require a substantial amount of annotated data or parallel translation data. We explore whether language representations that capture relationships among languages can be learned and subsequently leveraged in cross-lingual tasks without the use of parallel data. We generate dense embeddings for 29 languages using a denoising autoencoder, and evaluate the embeddings using the World Atlas of Language Structures (WALS) and two extrinsic tasks in a zero-shot setting: cross-lingual dependency parsing and cross-lingual natural language inference.",
    selected = true,
    abbr = {ACL},
}